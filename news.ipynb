{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Rollup bot\n",
    "\n",
    "Resources https://newsapi.org/docs/client-libraries/python and https://platform.openai.com/docs/api-reference/chat/create\n",
    "\n",
    "First get an api key and create a .env file using example.env - NewsAPI, OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "# pip3 install dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API = os.getenv('API')\n",
    "GPT = os.getenv('NEWS_GPT')\n",
    "GIT = os.getenv('GIT')\n",
    "GNEWS = os.getenv('GNEWS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'totalResults': 31, 'articles': [{'source': {'id': None, 'name': 'VentureBeat'}, 'author': 'Shubham Sharma', 'title': 'How Prophecy 3.0 enables businesses to build data pipelines without writing SQL code', 'description': 'Prophecy today launched a new version of its core platform to provide enterprises with low-code SQL capabilities for building data pipelines.', 'url': 'https://venturebeat.com/data-infrastructure/how-prophecy-3-0-enables-businesses-to-build-data-pipelines-without-writing-sql-code/', 'urlToImage': 'https://venturebeat.com/wp-content/uploads/2023/04/OIG-1-e1682521927193.jpeg?w=1200&strip=all', 'publishedAt': '2023-04-26T16:24:49Z', 'content': 'Join top executives in San Francisco on July 11-12, to hear how leaders are integrating and optimizing AI investments for success. Learn More\\r\\nCalifornia-based data engineering company Prophecy has a… [+1174 chars]'}, {'source': {'id': None, 'name': 'Forbes'}, 'author': 'Luis Marinelli, Forbes Councils Member, \\n Luis Marinelli, Forbes Councils Member\\n https://www.forbes.com/sites/forbesfinancecouncil/people/luismarinelli/', 'title': 'Managing Credit Risk For Fintechs: Innovative Strategies For A Data-Driven Future', 'description': 'To manage credit risk effectively, fintech lenders must adopt unique and insightful strategies that leverage data, analytics, machine learning and product design.', 'url': 'https://www.forbes.com/sites/forbesfinancecouncil/2023/04/26/managing-credit-risk-for-fintechs-innovative-strategies-for-a-data-driven-future/', 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/5f6b8b10fc4839cedc3fe5ba/0x0.jpg?format=jpg&width=1200', 'publishedAt': '2023-04-26T11:00:00Z', 'content': 'Luis Marinelli is a FinTech Entrepreneur. He is Co-founder at Vana. He Posts about Entrepreneurship, Finance &amp; Startups. \\r\\ngetty\\r\\nAs a powerful force in the financial landscape, fintechs offer in… [+6628 chars]'}, {'source': {'id': None, 'name': 'Forbes'}, 'author': 'Nirav Patel, Forbes Councils Member, \\n Nirav Patel, Forbes Councils Member\\n https://www.forbes.com/sites/forbestechcouncil/people/niravpatel/', 'title': 'The 5 Digital Supply Chain Challenges Every Business Should Plan For Before Their Next Fiscal Year', 'description': 'Ongoing supply chain challenges can be resolved by digital supply chain solutions that ensure visibility, information exchange, effective data aggregation, collaboration, analytics, automation, innovative practices and step-change improvements.', 'url': 'https://www.forbes.com/sites/forbestechcouncil/2023/04/26/the-5-digital-supply-chain-challenges-every-business-should-plan-for-before-their-next-fiscal-year/', 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/61950180a260c12d22db736c/0x0.jpg?format=jpg&width=1200', 'publishedAt': '2023-04-26T12:15:00Z', 'content': 'Nirav Patel is President and CEO, Bristlecone, leading the way in supply chain transformation and digital resilience.\\r\\ngetty\\r\\nSupply chains are complicated, typically consisting of a number of comple… [+6654 chars]'}, {'source': {'id': None, 'name': 'Forbes'}, 'author': 'David Prosser, Contributor, \\n David Prosser, Contributor\\n https://www.forbes.com/sites/davidprosser/', 'title': 'Unlocking Lending With Credora’s Privacy Preserving Credit Scoring', 'description': 'Credora has raised $6 million of new finance as demand for private credit continues to grow', 'url': 'https://www.forbes.com/sites/davidprosser/2023/04/25/unlocking-lending-with-credoras-privacy-preserving-credit-scoring/', 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/644786ca26b1a43633f33129/0x0.jpg?format=jpg&crop=675,380,x0,y86,safe&width=1200', 'publishedAt': '2023-04-25T11:00:40Z', 'content': 'Darshan Vaidya, CEO of Credora\\r\\ncredora\\r\\nHow do would-be borrowers share sensitive data with potential lenders so they can secure the credit they need for business growth? US based Credora, which is … [+4609 chars]'}, {'source': {'id': None, 'name': 'Forbes'}, 'author': 'Expert Panel®, Forbes Councils Member, \\n Expert Panel®, Forbes Councils Member\\n https://www.forbes.com/sites/forbesbusinesscouncil/people/expertpanel/', 'title': 'How Leaders Can Cut Supply Chain Costs And Improve Performance', 'description': 'Leaders that recognize the importance of data in decision making minimize risk and increases the likelihood of business success.', 'url': 'https://www.forbes.com/sites/forbesbusinesscouncil/2023/04/25/how-leaders-can-cut-supply-chain-costs-and-improve-performance/', 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/644036a0ff89147224eb47a2/0x0.jpg?format=jpg&width=1200', 'publishedAt': '2023-04-25T12:15:00Z', 'content': 'getty\\r\\nEfficient supply chain management is a crucial element of business success, impacting everything from customer satisfaction to profitability. With the global supply chain becoming increasingly… [+7367 chars]'}]}\n"
     ]
    }
   ],
   "source": [
    "# Use NewsAPI to get articles\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#set variables\n",
    "q = 'data analytics'\n",
    "domains = 'forbes.com, venturebeat.com, searchbusinessanalytics.techtarget.com, www.informationweek.com/big-data-analytics.asp, www.zdnet.com/topic/big-data-analytics, www.datasciencecentral.com, www.kdnuggets.com, www.analyticsinsight.net, www.datanami.com, Bloomberg' # comma seperated\n",
    "date_from = '2023-04-25'\n",
    "date_to = '2023-04-26'\n",
    "language = 'en'\n",
    "sortBy = 'relevancy' # relevancy, popularity, publishedAT\n",
    "pageSize = 5 # set max number of articles you'd like to summarize\n",
    "page = 1\n",
    "\n",
    "#make request\n",
    "# api_url = f\"https://newsapi.org/v2/everything?q={q}&apiKey={API}&from={date_from}&to={date_to}&language={language}&sortBy={sortBy}&pageSize={pageSize}&page={page}\"\n",
    "api_url = f\"https://newsapi.org/v2/everything?q={q}&apiKey={API}&domains={domains}&from={date_from}&to={date_to}&language={language}&sortBy={sortBy}&pageSize={pageSize}&page={page}\"\n",
    "response = requests.get(api_url)\n",
    "data = response.json()\n",
    "print(data)\n",
    "\n",
    "#write to tmp file\n",
    "with open('tmp_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How data brokers get your personal information.\n",
      "Description: And sell that data to the highest bidder.\n"
     ]
    }
   ],
   "source": [
    "# Testing GNews.io to replace NewsAPI\n",
    "\n",
    "# https://docs.python.org/3/library/json.html\n",
    "# This library will be used to parse the JSON data returned by the API.\n",
    "import json\n",
    "# https://docs.python.org/3/library/urllib.request.html#module-urllib.request\n",
    "# This library will be used to fetch the API.\n",
    "import urllib.request\n",
    "\n",
    "date = '2023-04-27'\n",
    "\n",
    "q = 'data&analytics'\n",
    "q_in = 'content'\n",
    "d_from = date\n",
    "d_to = date\n",
    "sortby = 'relevance'\n",
    "\n",
    "apikey = GNEWS\n",
    "url = f'https://gnews.io/api/v4/search?q={q}&lang=en&country=us&max=10&apikey={apikey}&from={d_from}&to={d_to}&sortby={sortby}&in={q_in}'\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    data = json.loads(response.read().decode(\"utf-8\"))\n",
    "    articles = data[\"articles\"]\n",
    "\n",
    "    for i in range(len(articles)):\n",
    "        # articles[i].title\n",
    "        print(f\"Title: {articles[i]['title']}\")\n",
    "        # articles[i].description\n",
    "        print(f\"Description: {articles[i]['description']}\")\n",
    "        # You can replace {property} below with any of the article properties returned by the API.\n",
    "        # articles[i].{property}\n",
    "        # print(f\"{articles[i]['{property}']}\")\n",
    "\n",
    "        # Delete this line to display all the articles returned by the request. Currently only the first article is displayed.\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing json load\n",
    "# with open('tmp_data.json') as data_tmp_file:\n",
    "#     data_string = data_tmp_file.read()\n",
    "# data = json.loads(data_string)\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse response for URLs and summarize with GPT\n",
    "# pip3 install openai\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# read and parse json from tmp\n",
    "with open('tmp_data.json') as data_tmp_file:\n",
    "    data_string = data_tmp_file.read()\n",
    "data = json.loads(data_string)\n",
    "    \n",
    "openai.api_key = GPT\n",
    "\n",
    "for i in data['articles']:\n",
    "    article = i['url']\n",
    "    content = f\"Provide a summary of this article in 3 bullet points (format 1, 2, 3). Condense the response and remove fillers like 'the article' and 'the author':{article}\"\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": content}])\n",
    "\n",
    "    # append summary(completion) to data\n",
    "    summary = completion.choices[0].message.content\n",
    "    i['summary'] = summary \n",
    "\n",
    "    # print(completion.choices[0].message.content)\n",
    "\n",
    "# write summarized output to file\n",
    "with open('tmp_summarized_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': {'id': None, 'name': 'Forbes'}, 'author': 'Bernd Greifeneder, Forbes Councils Member, \\n Bernd Greifeneder, Forbes Councils Member\\n https://www.forbes.com/sites/forbestechcouncil/people/berndgreifeneder/', 'title': 'Getting Tool Sprawl Under Control To Enable Data-Driven Business And Cloud-Scale Growth', 'description': 'Supported by a data lakehouse, organizations could unlock the value of the petabytes—and, eventually, yottabytes—of data they have available to access more precise real-time answers to the questions that matter. This can help them not just survive but thrive …', 'url': 'https://www.forbes.com/sites/forbestechcouncil/2023/04/24/getting-tool-sprawl-under-control-to-enable-data-driven-business-and-cloud-scale-growth/', 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/6399d93eaccb6518c9a6c001/0x0.jpg?format=jpg&width=1200', 'publishedAt': '2023-04-24T11:00:00Z', 'content': 'Bernd Greifeneder is the CTO and founder of Dynatrace, a software intelligence company that helps simplify enterprise cloud complexity.\\r\\ngetty\\r\\nBusiness leaders are being reminded once again that agi… [+6474 chars]', 'summary': '1. Enterprises are experiencing tool sprawl, leading to issues in data-driven decision making and cloud scalability. \\n2. The solution is to consolidate tools and invest in platforms that can address multiple needs, such as data governance and analytics. \\n3. Implementing a strong governance framework can also ensure that tools are being used effectively and efficiently.'}\n"
     ]
    }
   ],
   "source": [
    "# Testing for modifying and writing out json\n",
    "\n",
    "print(data['articles'][0])\n",
    "print(completion.choices[0].message.content)\n",
    "\n",
    "summary = completion.choices[0].message.content\n",
    "data['articles'][0]['summary'] = summary\n",
    "\n",
    "print(data['articles'][0])\n",
    "\n",
    "with open('tmp_summarized_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a summary of the latest news articles. Time to write that to a post. \n",
    "\n",
    "I currently have a portfolio site that creates blog posts out of markdown files so workflow should be:\n",
    "    1. Pull relevant information from tmp_summarized_data.json \n",
    "    2. Add a section for each news article including: Title, Source(Linked), Summary\n",
    "    3. Save that markdown file to a remote github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a markdown file to be used as the daily post\n",
    "\n",
    "with open('tmp_summarized_data.json') as data_tmp_file:\n",
    "    data_string = data_tmp_file.read()\n",
    "data = json.loads(data_string)\n",
    "\n",
    "# Write top of post file\n",
    "date = date_from\n",
    "topic = 'Data Analytics'\n",
    "\n",
    "top = f\"---\\ntitle: Rollup for {date}\\ndate: {date}\\ndescription: GPT generated news rollup for {date}\\ntag: {topic}\\nauthor: System\\n---\\n# {topic} news for {date}\\n\"\n",
    "\n",
    "# Parse data from summarized posts and put into markdown format\n",
    "post = ''\n",
    "\n",
    "for i in data['articles']:\n",
    "    #get variables from json\n",
    "    title = i['title']\n",
    "    source = i['source']['name']\n",
    "    url = i['url']\n",
    "    summary = i['summary']\n",
    "\n",
    "    #write variables to file\n",
    "    md_title = '### '+title\n",
    "    md_credit = '_['+source+']('+url+')_'\n",
    "    md_summary = summary\n",
    "    \n",
    "    post += f'{md_title}\\n{md_credit}\\n{md_summary}\\n'\n",
    "    \n",
    "# print(post)\n",
    "\n",
    "# write the markdown file\n",
    "f = open('tmp_newpost.mdx','w')\n",
    "f.write(\n",
    "    top +\n",
    "    post\n",
    ")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file to local source\n",
    "import os.path\n",
    "\n",
    "folder = '/Users/mattray/Documents/GitHub/mattray-xyz/pages/posts' #this is the folder hosting my blog\n",
    "file_name = date+'.mdx'\n",
    "file_path = os.path.join(folder, file_name)\n",
    "\n",
    "with open('tmp_newpost.mdx', 'r') as f1, open(file_path, 'w') as f2:\n",
    "    post = f1.read()\n",
    "    f2.write(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n",
      "b'{\"message\": \"my commit message\", \"content\": \"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\"}'\n"
     ]
    }
   ],
   "source": [
    "# Post request returns 201\n",
    "import requests\n",
    "from base64 import b64encode\n",
    "\n",
    "repo = 'mattray-xyz'\n",
    "path = 'pages/posts'\n",
    "owner = 'mdray'\n",
    "date = '2023-04-26'\n",
    "\n",
    "\n",
    "\n",
    "# Base64 endoce new post\n",
    "with open('tmp_newpost.mdx', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "data_bytes = data.encode('utf8')\n",
    "# print(data_bytes)\n",
    "data_b64 = b64encode(data_bytes)\n",
    "# print(data_b64)\n",
    "b64 = data_b64.decode('utf8')\n",
    "# print(b64)\n",
    "\n",
    "# Write GitHub API variables\n",
    "header = {'Authorization':'Bearer ' +GIT, \"Accept\": \"application/vnd.github+json\", \"X-GitHub-Api-Version\": \"2022-11-28\"}\n",
    "url = f'https://api.github.com/repos/{owner}/{repo}/contents/{path}/{date}.mdx'\n",
    "message = {'message':'my commit message', 'content':f'{b64}'}\n",
    "\n",
    "response = requests.put(url=url, headers=header, json=message)\n",
    "print(response)\n",
    "# print(response.request.url)\n",
    "print(response.request.body)\n",
    "# print(response.request.headers) # Contains passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Testing Github API\n",
    "# Get request returns 200\n",
    "import requests\n",
    "\n",
    "repo = 'mattray-xyz'\n",
    "path = 'pages/posts'\n",
    "owner = 'mdray'\n",
    "\n",
    "header = {'Authorization': GIT}\n",
    "url_get = f'https://api.github.com/repos/{owner}/{repo}/contents/{path}'\n",
    "message = {'message':'my commit message', 'content':'bXkgbmV3IGZpbGUgY29udGVudHM='}\n",
    "\n",
    "# print(message)\n",
    "\n",
    "# with open('tmp_newpost.mdx', 'r') as file:\n",
    "#     content = file.read()\n",
    "\n",
    "response = requests.get(url=url_get, headers=header)\n",
    "print(response)\n",
    "# print(response.request.url)\n",
    "# print(response.request.body)\n",
    "# print(response.request.headers) # Contains passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'test'\n",
      "b'dGVzdA=='\n",
      "dGVzdA==\n"
     ]
    }
   ],
   "source": [
    "# Playing with base64 encoding the post\n",
    "\n",
    "with open('tmp_newpost.mdx', 'r') as file:\n",
    "    post = file.read()\n",
    "# print(post)\n",
    "\n",
    "\n",
    "from base64 import b64encode\n",
    "data = \"test\"\n",
    "\n",
    "data_bytes = data.encode('utf8')\n",
    "print(data_bytes)\n",
    "\n",
    "data_b64 = b64encode(data_bytes)\n",
    "print(data_b64)\n",
    "\n",
    "b64 = data_b64.decode('utf8')\n",
    "print(b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-26\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "day = date.today() - timedelta(days = 1)\n",
    "print(day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
